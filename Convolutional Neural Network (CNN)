# -*- coding: utf-8 -*-
"""
Created on Sat Apr 26 16:34:20 2025

@author: proch
"""

import tensorflow as tf
from tensorflow.keras import datasets, layers, models, Sequential, Input
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
from tensorflow.keras.layers import LeakyReLU, RandomFlip, RandomRotation, RandomZoom
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau


# Load and preprocess data
(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()
train_images = train_images / 255.0
test_images  = test_images  / 255.0

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

# Display sample images
plt.figure(figsize=(10, 10))
for i in range(25):
    plt.subplot(5, 5, i + 1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i])
    plt.xlabel(class_names[train_labels[i][0]])
plt.show()

# Data augmentation layers
data_augmentation = tf.keras.Sequential([
    RandomFlip("horizontal"),
    RandomRotation(0.1),
    RandomZoom(0.1),
])

# Model definition
model = Sequential([
    Input(shape=(32, 32, 3)),
    data_augmentation,
    
    #Two conv layers per block with BatchNorm + LeakyReLU(Rectified Linear Unit)
    #Gradually increasing filter counts (32→64→128) to captures more features.
    layers.Conv2D(64, (3, 3), padding="same"),  #conv 1
    LeakyReLU(alpha=0.01),
    layers.BatchNormalization(),                #BatchNorm
    layers.Conv2D(64, (3, 3), padding="same"),  #conv2
    LeakyReLU(alpha=0.01),  
    layers.BatchNormalization(),
    layers.MaxPooling2D((2, 2)),                
    layers.Dropout(0.2),

    layers.Conv2D(128, (3, 3), padding="same"),
    LeakyReLU(alpha=0.01),  
    layers.BatchNormalization(),
    layers.Conv2D(128, (3, 3), padding="same"),
    LeakyReLU(alpha=0.01),  
    layers.BatchNormalization(),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.3),

    layers.Conv2D(128, (3, 3), padding="same"),
    LeakyReLU(alpha=0.01),  
    layers.BatchNormalization(),
    layers.Conv2D(128, (3, 3), padding="same"),
    LeakyReLU(alpha=0.01),  
    layers.BatchNormalization(),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.4),

    layers.Flatten(),
    layers.Dense(128),
    LeakyReLU(alpha=0.01),  
    layers.BatchNormalization(),
    layers.Dropout(0.4),
    layers.Dense(10, activation="softmax")
])

opt = Adam(learning_rate=0.001)
model.compile(optimizer=opt, loss="sparse_categorical_crossentropy", metrics=["accuracy"])

# Model summary
model.summary()

callbacks = [
    EarlyStopping(patience=5, restore_best_weights=True),
    ReduceLROnPlateau(factor=0.5, patience=3)
]

# Single fit call, with callbacks
history = model.fit(
    train_images,
    train_labels,
    epochs=50,
    batch_size=64,        # larger batch for faster GPU throughput    
    validation_data=(test_images, test_labels),
    callbacks=callbacks
)



# Plot training history
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.5, 1])
plt.legend(loc='lower right')
plt.show()


# Evaluate the model
test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)
print(f'Test accuracy: {test_acc}')

# Save trained model
model.save('Pisces_CIFARmodel.h5')
